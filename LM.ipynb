{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67402e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEMO: Manual constraint queries\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Manual queries:  25%|██▌       | 1/4 [01:59<05:59, 119.81s/query, canonical=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constraints: {'num_params': 120000, 'num_flops': 3000000, 'epoch_time_sec': 30.0}\n",
      "Predicted architecture: Conv2d(k=3,p=1) -> Conv2d(k=3,p=1) -> KAN(order=4,grid=5,act=ReLU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Manual queries:  50%|█████     | 2/4 [02:52<02:41, 80.55s/query, canonical=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constraints: {'num_params': 700000, 'num_flops': 25000000, 'epoch_time_sec': 40.0}\n",
      "Predicted architecture: Conv2d(k=3,p=1) -> Conv2d(k=5,p=1) -> Conv2d(k=7,p=1) -> KAN(order=4,grid=8,act=SiLU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Manual queries:  75%|███████▌  | 3/4 [03:52<01:11, 71.09s/query, canonical=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constraints: {'num_params': 2000000, 'num_flops': 90000000, 'epoch_time_sec': 70.0}\n",
      "Predicted architecture: Conv2d(k=3,p=1) -> Conv2d(k=3,p=1) -> Conv2d(k=5,p=1) -> Conv2d(k=7,p=1) -> KAN(order=4,grid=8,act=SiLU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Manual queries: 100%|██████████| 4/4 [04:44<00:00, 71.22s/query, canonical=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constraints: {'num_params': 5000000, 'num_flops': 200000000, 'epoch_time_sec': 200.0}\n",
      "Predicted architecture: Conv2d(k=3,p=1) -> Conv2d(k=5,p=1) -> Conv2d(k=7,p=1) -> KAN(order=4,grid=8,act=SiLU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral:7b: 100%|██████████| 200/200 [1:36:28<00:00, 28.94s/sample, exact_avg=0.000, jac_avg=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Time: 5788.4s\n",
      "Exact-match accuracy: 0.0000\n",
      "Mean token-Jaccard:  0.2506\n",
      "Saved: nas_arch_predictions_mistral_7b.csv\n",
      "\n",
      "Final Results:\n",
      "mistral:7b: exact=0.0000, jaccard=0.2506, time=5788.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"csv_file\": \"nas_cnn_kan_cifar100_results.csv\",\n",
    "    \"n_samples_to_load\": 5000,\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"n_test_samples\": 200,\n",
    "    \"ollama_url\": \"http://localhost:11434\",\n",
    "    \"timeout\": 1200,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_predict\": 120,\n",
    "    \"max_tries_format\": 3,\n",
    "    \"lora_adapter_dir\": \"nas_mistral7b_lora\",\n",
    "    \"fine_tuned_model_name\": \"nas-mistral-lora\",\n",
    "}\n",
    "\n",
    "KAN_ACTS = {\"RELU\": \"ReLU\", \"GELU\": \"GELU\", \"SILU\": \"SiLU\"}\n",
    "\n",
    "def load_data(csv_file, n_samples=None):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if n_samples is not None and n_samples < len(df):\n",
    "        df = df.sample(n=n_samples, random_state=CONFIG[\"random_state\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def normalize_arch(text: str) -> str:\n",
    "    s = \" \".join(str(text).strip().split())\n",
    "    s = s.replace(\"→\", \"->\")\n",
    "    s = s.replace(\" - \", \" -> \")\n",
    "    s = s.replace(\"->\", \" -> \")\n",
    "    s = re.sub(r\"\\s+->\\\\s+\", \" -> \", s)\n",
    "    for prefix in [\"ARCHITECTURE:\", \"Architecture:\", \"architecture:\"]:\n",
    "        if s.startswith(prefix):\n",
    "            s = s[len(prefix):].strip()\n",
    "    s = re.sub(r\"Conv2dk(\\d+)\\s*,\\s*p(\\d+)\", r\"Conv2d(k=\\1,p=\\2)\", s, flags=re.I)\n",
    "    s = re.sub(r\"Conv2d\\s*\\(\\s*k\\s*=\\s*(\\d+)\\s*,\\s*p\\s*=\\s*(\\d+)\\s*\\)\",\n",
    "               r\"Conv2d(k=\\1,p=\\2)\", s, flags=re.I)\n",
    "    m = re.search(r\"KAN\\s*order\\s*(\\d+)\\s*,\\s*grid\\s*(\\d+)\\s*,\\s*act\\s*([A-Za-z0-9_]+)\", s, flags=re.I)\n",
    "    if m:\n",
    "        order, grid, act = m.group(1), m.group(2), m.group(3).upper()\n",
    "        act = KAN_ACTS.get(act, act.title())\n",
    "        s = re.sub(\n",
    "            r\"KAN\\s*order\\s*\\d+\\s*,\\s*grid\\s*\\d+\\s*,\\s*act\\s*[A-Za-z0-9_]+\",\n",
    "            f\"KAN(order={order},grid={grid},act={act})\",\n",
    "            s, flags=re.I\n",
    "        )\n",
    "    s = re.sub(\n",
    "        r\"KAN\\s*\\(\\s*order\\s*=\\s*(\\d+)\\s*,\\s*grid\\s*=\\s*(\\d+)\\s*,\\s*act\\s*=\\s*([A-Za-z]+)\\s*\\)\",\n",
    "        lambda x: f\"KAN(order={x.group(1)},grid={x.group(2)},act={KAN_ACTS.get(x.group(3).upper(), x.group(3))})\",\n",
    "        s\n",
    "    )\n",
    "    return s.strip()\n",
    "\n",
    "def is_canonical_arch(s: str) -> bool:\n",
    "    conv = r\"(?:Conv2d\\(k=\\d+,p=\\d+\\))(?: -> Conv2d\\(k=\\d+,p=\\d+\\))*\"\n",
    "    kan  = r\"KAN\\(order=\\d+,grid=\\d+,act=(?:ReLU|GELU|SiLU)\\)\"\n",
    "    return re.fullmatch(conv + r\" -> \" + kan, s) is not None\n",
    "\n",
    "def serialize_constraints_from_row(row):\n",
    "    acc_weight = float(row.get('acc_weight', 0.8))\n",
    "    eff_weight = 1.0 - acc_weight\n",
    "    return (\n",
    "        \"Goal: choose an architecture by trading off CIFAR-100 test_accuracy vs efficiency.\\\\n\"\n",
    "        \"Trade-off:\\\\n\"\n",
    "        f\"- acc_weight = {acc_weight:.3f} (higher favors accuracy)\\\\n\"\n",
    "        f\"- eff_weight = {eff_weight:.3f} (higher favors efficiency)\\\\n\"\n",
    "        \"Constraints (must satisfy all):\\\\n\"\n",
    "        f\"- num_params <= {int(row['num_params'])}\\\\n\"\n",
    "        f\"- num_flops <= {int(row['num_flops'])}\\\\n\"\n",
    "        f\"- epoch_time_sec <= {float(row['epoch_time_sec']):.3f}\\\\n\"\n",
    "        \"Output ONLY one architecture string in this exact format:\\\\n\"\n",
    "        \"Conv2d(k=<int>,p=<int>) -> ... -> KAN(order=<int>,grid=<int>,act=<ReLU|GELU|SiLU>)\"\n",
    "    )\n",
    "\n",
    "def serialize_constraints_manual(num_params, num_flops, epoch_time_sec, acc_weight=0.8):\n",
    "    eff_weight = 1.0 - float(acc_weight)\n",
    "    return (\n",
    "        \"Goal: choose an architecture by trading off CIFAR-100 test_accuracy vs efficiency.\\\\n\"\n",
    "        \"Trade-off:\\\\n\"\n",
    "        f\"- acc_weight = {float(acc_weight):.3f} (higher favors accuracy)\\\\n\"\n",
    "        f\"- eff_weight = {eff_weight:.3f} (higher favors efficiency)\\\\n\"\n",
    "        \"Constraints (must satisfy all):\\\\n\"\n",
    "        f\"- num_params <= {int(num_params)}\\\\n\"\n",
    "        f\"- num_flops <= {int(num_flops)}\\\\n\"\n",
    "        f\"- epoch_time_sec <= {float(epoch_time_sec):.3f}\\\\n\"\n",
    "        \"Output ONLY one architecture string in this exact format:\\\\n\"\n",
    "        \"Conv2d(k=<int>,p=<int>) -> ... -> KAN(order=<int>,grid=<int>,act=<ReLU|GELU|SiLU>)\"\n",
    "    )\n",
    "\n",
    "def build_prompt(constraint_text):\n",
    "    return f\"<s>[INST] {constraint_text} [/INST]\"\n",
    "\n",
    "def generate_raw(prompt, model_name, config):\n",
    "    r = requests.post(\n",
    "        f\"{config['ollama_url']}/api/generate\",\n",
    "        json={\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": config[\"temperature\"],\n",
    "                \"top_p\": config[\"top_p\"],\n",
    "                \"num_predict\": config[\"num_predict\"],\n",
    "            },\n",
    "        },\n",
    "        timeout=config[\"timeout\"],\n",
    "    )\n",
    "    return r.json()[\"response\"].strip()\n",
    "\n",
    "def predict_architecture_canonical_from_text(constraint_text, model_name, config):\n",
    "    last_raw, last_norm = \"\", \"\"\n",
    "    for _ in range(config[\"max_tries_format\"]):\n",
    "        prompt = build_prompt(constraint_text)\n",
    "        raw = generate_raw(prompt, model_name, config)\n",
    "        norm = normalize_arch(raw)\n",
    "        last_raw, last_norm = raw, norm\n",
    "        if is_canonical_arch(norm):\n",
    "            return norm, raw\n",
    "    return last_norm, last_norm\n",
    "\n",
    "def predict_architecture_canonical_from_row(row, model_name, config):\n",
    "    constraint_text = serialize_constraints_from_row(row)\n",
    "    return predict_architecture_canonical_from_text(constraint_text, model_name, config)\n",
    "\n",
    "def token_jaccard(a, b):\n",
    "    a = str(a).replace(\"->\", \" \").replace(\",\", \" \")\n",
    "    b = str(b).replace(\"->\", \" \").replace(\",\", \" \")\n",
    "    A = set(a.split())\n",
    "    B = set(b.split())\n",
    "    if len(A | B) == 0:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "def evaluate_model(model_name, test_df, config):\n",
    "    n = min(config[\"n_test_samples\"], len(test_df))\n",
    "    test_subset = test_df.sample(n=n, random_state=config[\"random_state\"]).reset_index(drop=True)\n",
    "    preds, raws, exacts, jacs = [], [], [], []\n",
    "    start = datetime.now()\n",
    "    pbar = tqdm(total=n, desc=f\"Evaluating {model_name}\", unit=\"sample\", dynamic_ncols=True)\n",
    "    for i in range(n):\n",
    "        row = test_subset.iloc[i]\n",
    "        pred_arch, raw = predict_architecture_canonical_from_row(row, model_name, config)\n",
    "        true_arch = str(row[\"architecture\"]).strip()\n",
    "        preds.append(pred_arch)\n",
    "        raws.append(raw)\n",
    "        exacts.append(int(pred_arch == true_arch))\n",
    "        jacs.append(token_jaccard(pred_arch, true_arch))\n",
    "        pbar.set_postfix({\n",
    "            \"exact_avg\": f\"{np.mean(exacts):.3f}\",\n",
    "            \"jac_avg\": f\"{np.mean(jacs):.3f}\",\n",
    "        })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    exact_acc = float(np.mean(exacts))\n",
    "    mean_jac = float(np.mean(jacs))\n",
    "    print(f\"\\\\nExecution Time: {elapsed:.1f}s\")\n",
    "    print(f\"Exact-match accuracy: {exact_acc:.4f}\")\n",
    "    print(f\"Mean token-Jaccard:  {mean_jac:.4f}\")\n",
    "    out = test_subset.copy()\n",
    "    out[\"predicted_architecture\"] = preds\n",
    "    out[\"raw_model_output\"] = raws\n",
    "    out[\"exact_match\"] = exacts\n",
    "    out[\"token_jaccard\"] = jacs\n",
    "    out_csv = f\"nas_arch_predictions_{model_name.replace(':','_')}.csv\"\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "    return {\"model\": model_name, \"exact_acc\": exact_acc, \"mean_jaccard\": mean_jac, \"elapsed_sec\": elapsed}\n",
    "\n",
    "def demo_manual_constraints(model_name, config):\n",
    "    demo_constraints = [\n",
    "        {\"num_params\": 120_000,   \"num_flops\": 3_000_000,   \"epoch_time_sec\": 30.0, \"acc_weight\": 0.8},\n",
    "        {\"num_params\": 700_000,   \"num_flops\": 25_000_000,  \"epoch_time_sec\": 40.0, \"acc_weight\": 0.6},\n",
    "        {\"num_params\": 2_000_000, \"num_flops\": 90_000_000,  \"epoch_time_sec\": 70.0, \"acc_weight\": 0.9},\n",
    "        {\"num_params\": 5_000_000, \"num_flops\": 200_000_000, \"epoch_time_sec\": 200.0, \"acc_weight\": 0.5},\n",
    "    ]\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"DEMO: Manual constraint queries\")\n",
    "    print(\"=\" * 80)\n",
    "    pbar = tqdm(demo_constraints, desc=\"Manual queries\", unit=\"query\", dynamic_ncols=True)\n",
    "    for c in pbar:\n",
    "        constraint_text = serialize_constraints_manual(c[\"num_params\"], c[\"num_flops\"], c[\"epoch_time_sec\"], c[\"acc_weight\"])\n",
    "        pred, raw = predict_architecture_canonical_from_text(constraint_text, model_name, config)\n",
    "        ok = is_canonical_arch(pred)\n",
    "        pbar.set_postfix({\"canonical\": ok})\n",
    "        print(\"\\\\nConstraints:\", c)\n",
    "        print(\"Predicted architecture:\", pred)\n",
    "        if not ok:\n",
    "            print(\"Raw model output:\", raw)\n",
    "\n",
    "def setup_ollama_model(config):\n",
    "    modelfile_content = f\"\"\"FROM mistral:7b\n",
    "ADAPTER ./{config['lora_adapter_dir']}\"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"Setting up fine-tuned model in Ollama\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Modelfile content:\")\n",
    "    print(modelfile_content)\n",
    "    print()\n",
    "    \n",
    "    r = requests.post(\n",
    "        f\"{config['ollama_url']}/api/create\",\n",
    "        json={\n",
    "            \"name\": config['fine_tuned_model_name'],\n",
    "            \"modelfile\": modelfile_content,\n",
    "            \"stream\": False\n",
    "        },\n",
    "        timeout=300\n",
    "    )\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        print(f\"✓ Model '{config['fine_tuned_model_name']}' created successfully\")\n",
    "    else:\n",
    "        print(f\"Model creation response: {r.status_code}\")\n",
    "    \n",
    "    return config['fine_tuned_model_name']\n",
    "\n",
    "def main():\n",
    "    df = load_data(CONFIG[\"csv_file\"], n_samples=CONFIG[\"n_samples_to_load\"])\n",
    "    bins = pd.qcut(df[\"test_accuracy\"], q=10, duplicates=\"drop\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=CONFIG[\"test_size\"],\n",
    "        random_state=CONFIG[\"random_state\"],\n",
    "        stratify=bins,\n",
    "    )\n",
    "    \n",
    "    model_name = setup_ollama_model(CONFIG)\n",
    "    \n",
    "    demo_manual_constraints(model_name, CONFIG)\n",
    "    \n",
    "    results = []\n",
    "    results.append(evaluate_model(model_name, test_df, CONFIG))\n",
    "    \n",
    "    print(\"\\\\nFinal Results:\")\n",
    "    for r in results:\n",
    "        print(f\"{r['model']}: exact={r['exact_acc']:.4f}, jaccard={r['mean_jaccard']:.4f}, time={r['elapsed_sec']:.1f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
